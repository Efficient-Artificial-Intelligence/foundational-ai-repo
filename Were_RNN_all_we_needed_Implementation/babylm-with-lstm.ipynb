{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Implementation with MinLSTM**","metadata":{}},{"cell_type":"markdown","source":"## **Sequential Mode**","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nclass MinLSTMinSequentialMode:\n    def __init__(self, input_size, hidden_size):\n        self.hidden_size = hidden_size\n\n        # Initialize weights for gates\n        self.Wf = np.random.randn(input_size, hidden_size) * 0.01  # Forget gate weights\n        self.Wi = np.random.randn(input_size, hidden_size) * 0.01  # Input gate weights\n        self.Wc = np.random.randn(input_size, hidden_size) * 0.01  # Cell candidate weights\n        self.Wo = np.random.randn(input_size, hidden_size) * 0.01  # Output gate weights\n\n        self.Uf = np.random.randn(hidden_size, hidden_size) * 0.01  # Forget gate recurrent weights\n        self.Ui = np.random.randn(hidden_size, hidden_size) * 0.01  # Input gate recurrent weights\n        self.Uc = np.random.randn(hidden_size, hidden_size) * 0.01  # Cell candidate recurrent weights\n        self.Uo = np.random.randn(hidden_size, hidden_size) * 0.01  # Output gate recurrent weights\n\n        # Initialize biases\n        self.bf = np.zeros((1, hidden_size))\n        self.bi = np.zeros((1, hidden_size))\n        self.bc = np.zeros((1, hidden_size))\n        self.bo = np.zeros((1, hidden_size))\n\n    def forward_sequence(self, x_sequence):\n        \"\"\"\n        Process an entire sequence in sequential mode\n        \n        Args:\n        x_sequence (numpy.ndarray): Input sequence of shape (seq_length, input_size)\n        \n        Returns:\n        tuple: Hidden states and cell states for the entire sequence\n        \"\"\"\n        seq_length = x_sequence.shape[0]\n        h_states = np.zeros((seq_length, self.hidden_size))\n        c_states = np.zeros((seq_length, self.hidden_size))\n\n        # Initial hidden and cell states\n        h_prev = np.zeros((1, self.hidden_size))\n        c_prev = np.zeros((1, self.hidden_size))\n\n        # Process each time step sequentially\n        for t in range(seq_length):\n            # Current input at time step t\n            x_t = x_sequence[t:t+1]\n\n            # Forget gate\n            f_t = self.sigmoid(np.dot(x_t, self.Wf) + np.dot(h_prev, self.Uf) + self.bf)\n            \n            # Input gate\n            i_t = self.sigmoid(np.dot(x_t, self.Wi) + np.dot(h_prev, self.Ui) + self.bi)\n            \n            # Cell candidate\n            c_tilde = np.tanh(np.dot(x_t, self.Wc) + np.dot(h_prev, self.Uc) + self.bc)\n            \n            # New cell state\n            c_t = f_t * c_prev + i_t * c_tilde\n            \n            # Output gate\n            o_t = self.sigmoid(np.dot(x_t, self.Wo) + np.dot(h_prev, self.Uo) + self.bo)\n            \n            # New hidden state\n            h_t = o_t * np.tanh(c_t)\n\n            # Store states\n            h_states[t] = h_t\n            c_states[t] = c_t\n\n            # Update previous states for next iteration\n            h_prev = h_t\n            c_prev = c_t\n\n        return h_states, c_states\n\n    def forward_single_step(self, x_t, h_prev, c_prev):\n        \"\"\"\n        Process a single time step\n        \n        Args:\n        x_t (numpy.ndarray): Input at current time step\n        h_prev (numpy.ndarray): Previous hidden state\n        c_prev (numpy.ndarray): Previous cell state\n        \n        Returns:\n        tuple: Current hidden state and cell state\n        \"\"\"\n        # Forget gate\n        f_t = self.sigmoid(np.dot(x_t, self.Wf) + np.dot(h_prev, self.Uf) + self.bf)\n        \n        # Input gate\n        i_t = self.sigmoid(np.dot(x_t, self.Wi) + np.dot(h_prev, self.Ui) + self.bi)\n        \n        # Cell candidate\n        c_tilde = np.tanh(np.dot(x_t, self.Wc) + np.dot(h_prev, self.Uc) + self.bc)\n        \n        # New cell state\n        c_t = f_t * c_prev + i_t * c_tilde\n        \n        # Output gate\n        o_t = self.sigmoid(np.dot(x_t, self.Wo) + np.dot(h_prev, self.Uo) + self.bo)\n        \n        # New hidden state\n        h_t = o_t * np.tanh(c_t)\n\n        return h_t, c_t\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:51:41.969994Z","iopub.execute_input":"2024-12-03T20:51:41.970486Z","iopub.status.idle":"2024-12-03T20:51:41.989719Z","shell.execute_reply.started":"2024-12-03T20:51:41.970448Z","shell.execute_reply":"2024-12-03T20:51:41.988409Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## **Parallel Mode** ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nclass MinLSTMinParallelMode:\n    def __init__(self, input_size, hidden_size):\n        self.hidden_size = hidden_size\n\n        # Initialize weights\n        self.Wf = np.random.randn(input_size, hidden_size) * 0.01  # Forget gate weights\n        self.Wi = np.random.randn(input_size, hidden_size) * 0.01  # Input gate weights\n        self.Wc = np.random.randn(input_size, hidden_size) * 0.01  # Cell state weights\n        self.Wo = np.random.randn(input_size, hidden_size) * 0.01  # Output gate weights\n\n        self.Uf = np.random.randn(hidden_size, hidden_size) * 0.01  # Forget gate recurrent weights\n        self.Ui = np.random.randn(hidden_size, hidden_size) * 0.01  # Input gate recurrent weights\n        self.Uc = np.random.randn(hidden_size, hidden_size) * 0.01  # Cell state recurrent weights\n        self.Uo = np.random.randn(hidden_size, hidden_size) * 0.01  # Output gate recurrent weights\n\n        # Initialize biases\n        self.bf = np.zeros((1, hidden_size))\n        self.bi = np.zeros((1, hidden_size))\n        self.bc = np.zeros((1, hidden_size))\n        self.bo = np.zeros((1, hidden_size))\n\n    def parallel_scan(self, x_seq):\n        n = x_seq.shape[0]\n        h_states = np.zeros((n, self.hidden_size))\n        c_states = np.zeros((n, self.hidden_size))\n\n        for t in range(n):\n            if t == 0:\n                h_prev = np.zeros((self.hidden_size,))  # Initial hidden state\n                c_prev = np.zeros((self.hidden_size,))  # Initial cell state\n            else:\n                h_prev = h_states[t-1]  # Previous hidden state\n                c_prev = c_states[t-1]  # Previous cell state\n\n            f_t = self.sigmoid(np.dot(x_seq[t], self.Wf) + np.dot(h_prev, self.Uf) + self.bf)\n            i_t = self.sigmoid(np.dot(x_seq[t], self.Wi) + np.dot(h_prev, self.Ui) + self.bi)\n            c_tilde = np.tanh(np.dot(x_seq[t], self.Wc) + np.dot(h_prev, self.Uc) + self.bc)\n            c_t = f_t * c_prev + i_t * c_tilde\n            o_t = self.sigmoid(np.dot(x_seq[t], self.Wo) + np.dot(h_prev, self.Uo) + self.bo)\n            h_states[t] = o_t * np.tanh(c_t)\n            c_states[t] = c_t\n\n        return h_states, c_states\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:51:26.905848Z","iopub.execute_input":"2024-12-03T20:51:26.906316Z","iopub.status.idle":"2024-12-03T20:51:26.920065Z","shell.execute_reply.started":"2024-12-03T20:51:26.906280Z","shell.execute_reply":"2024-12-03T20:51:26.918847Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## **BabyLM with MinLSTM**","metadata":{}},{"cell_type":"code","source":"class BabyLMwithMinLSTM:\n    def __init__(self, vocab_size, embedding_size, hidden_size):\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n\n        # Embedding layer\n        self.embedding = np.random.randn(vocab_size, embedding_size) * 0.01\n\n        # MinLSTM Layer\n        self.min_lstm = MinLSTMinParallelMode(embedding_size, hidden_size)\n\n        # Output layer\n        self.Wo = np.random.randn(hidden_size, vocab_size) * 0.01  # Output weights\n        self.bo = np.zeros((1, vocab_size))  # Output biases\n\n    def forward(self, input_seq):\n        # Step 1: Get the embeddings of the input sequence\n        embedded_seq = self.embedding[input_seq]\n\n        # Step 2: Pass the embedded sequence through the MinLSTM\n        h_states, _ = self.min_lstm.parallel_scan(embedded_seq)\n\n        # Step 3: Compute the logits (pre-softmax output)\n        logits = np.dot(h_states, self.Wo) + self.bo\n\n        return logits\n\n    def predict(self, input_seq):\n        # Forward pass\n        logits = self.forward(input_seq)\n\n        # Softmax to get probabilities of the next word in the sequence\n        probabilities = self.softmax(logits[-1])\n\n        # Return the index of the word with the highest probability (next word)\n        return np.argmax(probabilities)\n\n    def softmax(self, x):\n        exp_x = np.exp(x - np.max(x))  # For numerical stability\n        return exp_x / np.sum(exp_x)\n\n# Example usage with words\n# Vocabulary and mapping of words to indices\nvocab = {\n    \"hello\": 0,\n    \"world\": 1,\n    \"this\": 2,\n    \"is\": 3,\n    \"test\": 4\n}\nvocab_size = len(vocab)\ninv_vocab = {v: k for k, v in vocab.items()}  # For decoding indices back to words\n\n# Hyperparameters\nembedding_size = 5  # Embedding size\nhidden_size = 4  # Hidden state size for LSTM\n\n# Instantiate BabyLM_LSTM\nbaby_lm_lstm = BabyLMwithMinLSTM(vocab_size, embedding_size, hidden_size)\n\n# Example input sequence (words converted to indices)\ninput_words = [\"hello\", \"world\", \"this\", \"is\"]\ninput_seq = np.array([vocab[word] for word in input_words])\n\n# Predict the next word\nnext_word_idx = baby_lm_lstm.predict(input_seq)\nnext_word = inv_vocab[next_word_idx]\n\nprint(\"Input words:\", input_words)\nprint(\"Predicted next word:\", next_word)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:52:18.920816Z","iopub.execute_input":"2024-12-03T20:52:18.921256Z","iopub.status.idle":"2024-12-03T20:52:18.936390Z","shell.execute_reply.started":"2024-12-03T20:52:18.921221Z","shell.execute_reply":"2024-12-03T20:52:18.934834Z"}},"outputs":[{"name":"stdout","text":"Input words: ['hello', 'world', 'this', 'is']\nPredicted next word: hello\n","output_type":"stream"}],"execution_count":14}]}